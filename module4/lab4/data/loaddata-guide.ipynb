{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a how to guide for importing data from .csv file to Django database (SQLite). You will see example data classes. Please note that those classes are for demonstration only. You are free to create classes in the way you see fit the best. \n",
    "\n",
    "The end goal is to create a JSON data that will be used to import data by Django built-in command. The format we want is structured as below. (more detail check -> https://docs.djangoproject.com/en/5.1/howto/initial-data/)\n",
    "\n",
    "```JSON\n",
    "[\n",
    "  {\n",
    "    \"model\": \"myapp.classname\",\n",
    "    \"pk\": 1,\n",
    "    \"fields\": {\n",
    "      \"attribute_name1\": \"Value\",\n",
    "      \"attribute_name2\": \"Value\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"model\": \"myapp.field\",\n",
    "    \"pk\": 2,\n",
    "    \"fields\": {\n",
    "      \"field_id\": \"32\",\n",
    "      \"field_name\": \"Corner\"\n",
    "    }\n",
    "  }\n",
    "]\n",
    "\n",
    "```\n",
    "\n",
    "In Python, you can assume that JSON is Dictionary-like data type. So, the overall step is .csv -> Pandas dataframe -> dictionary ->JSON. \n",
    "\n",
    "Let's start with importing necessary modules. In this case, we only need 2 modules. Then load data from .csv to Pandas dataframe (we did this before in lab 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python's module\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "log_df = pd.read_csv(\"operation-log.csv\")  \n",
    "log_df\n",
    "# _df stands for data frame. It is a common suffix to indicate variable type. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First data class I want to create is Operator class ass below. As you can see that I created a class with attributes that we don't have data. That is okay, we can leave those attributes optional by adding `blank=True, null=True` (more detail here -> https://docs.djangoproject.com/en/5.1/ref/models/fields/#field-options).\n",
    "\n",
    "```python \n",
    "class Operator(models.Model):\n",
    "    firstname = models.CharField(max_length=30)\n",
    "    lastname = models.CharField(max_length=30, blank=True, null=True)\n",
    "    middlename = models.CharField(max_length=30, blank=True, null=True)\n",
    "    phone = models.CharField(max_length=10, blank=True, null=True)\n",
    "```\n",
    "\n",
    "You can think of a database class as a table. So, let's create an `Operator` table (dataframe). We will use Pandas' Unique fucntion to see how many operator rows we need to create. (more detail about unique function -> https://pandas.pydata.org/docs/reference/api/pandas.unique.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df['operator'].unique() # find a unique value in dataframe's `operator` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result from unique function is an array which is iterable. We will iterate over the array and create a dataframe. There are several ways to create a dataframe. But I recommend doing by using a list of dict or a dict of list. I will show you the first case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list which we will add dictionaries later\n",
    "operators = [] # suffix s is common way to indicate a list (array)\n",
    "for person in log_df['operator'].unique(): # we iterate over the result from unique function\n",
    "    operators.append( # append new element to the list\n",
    "        { # curly bracket indicate the starting of dictionary\n",
    "            \"firstname\": person, # each key-value pair\n",
    "            \"lastname\": \"\",\n",
    "            \"phone\": \"\"\n",
    "        } # the end of dictionary\n",
    "    )\n",
    "operators # check the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of dictionaries. Making a dataframe is just one function. Note that I leave `lastname` and `phone` emtyp. This is an explicit way to work with optional attribute. You can also just ignore that attribute like I did with `middlename`. You will get the same outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe\n",
    "operator_df = pd.DataFrame(operators)\n",
    "operator_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to structure the data as required (check data format above). We will need to add `model` which is the class's name in the database. And `pk` or primary key which is the row's index. We want JSON at the end, but JSON is pretty much like dictionary in Python. So, we just need to create a (list of) dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operators = [] # create an empty list\n",
    "for index, row in operator_df.iterrows(): # we iterate over the dataframe row by row\n",
    "    operators.append( # append new element to the list\n",
    "        { # curly bracket indicate the starting of dictionary\n",
    "            \"model\": \"acrelog.operator\",  # the class name\n",
    "            \"pk\": index+1, # index starts at 0 while primary key starts at 1. \n",
    "            \"fields\": { # this is a dictionary of attributes\n",
    "                \"firstname\": row['firstname'] # only need firstname for now\n",
    "            }\n",
    "        } # the end of dictionary\n",
    "    )\n",
    "operators # see the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data in the format that we need. To convert into JSON, you can just run the following command. Then you can copy the result and check with https://jsonlint.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(operators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or write the JSON to the file directly \n",
    "with open('operator-data.json', 'w') as fp:\n",
    "    json.dump(operators, fp=fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might wonder why we create a dictionary then convert into a dataframe just for turn it into a dictionary again. Can you just make a dictionary that follows the required stucture directly? Absolutely yes. In some cases, that will be the better way to deal with the data. But there is a usecase for a dataframe as well. You will see it soon. For now, let's create another data class. \n",
    "\n",
    "```python\n",
    "class Operation(models.Model):\n",
    "    date = models.DateField()\n",
    "    note = models.CharField(max_length=300, blank=True, null=True)\n",
    "    operator = models.ForeignKey(Operator, on_delete=models.CASCADE)\n",
    "```\n",
    "In this class, it has a foreignkey that points to Operator class. So, instead of recording operator name, we will keep the primary key that points to that operator. For better understanding, I will trim the original dataframe to keep only the data that we will play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can subset the dataframe with any columns (and order)\n",
    "sub_log_df = log_df[['date', 'operator', 'note']]\n",
    "sub_log_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the map function (more detail -> https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html) to map from the operator's name to primary key. The map functions take several options. But we will use dictionary. The dictionary we want has operator's names as keys and primary keys as values. Do you feel that this structure looks familar. Yes, that is the operator dataframe that we created earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operator_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned earlier, dataframe is made from a list of dictionaries or a dictionary of lists. We can convert it back as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operator_df.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format you see above is a dictionary of list. We want to use firstname to map with primary key (or dataframe's index). So we will only care about the `firstname`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operator_df.to_dict()['firstname']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost done. We want a dictionary that keys are firstname and values are primary keys. But what we got is opposite. No problem, we just need to inverse it (and add 1 to pandas' index to make it become primary key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = operator_df.to_dict()['firstname']\n",
    "name2pk = dict((v, k+1) for k, v in index_name.items())\n",
    "name2pk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got it. Now, let map the operator's name to primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_log_df['operator'] = sub_log_df['operator'].map(name2pk) \n",
    "sub_log_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we make the final dictionary, we need to deal with date format. Django's SQLite expects `YYYY-MM-DD` format. So, we need to fix our date column a little bit. What you see is only string (a series of charactor). Python has a date (or datetime) variable for dealing with this type of data. Date data type is numerial. So, we can do calulation and also, in our case, format in the way we want. First, we need to convert string to datetime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(sub_log_df['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have a date column which we can do many thing on it. But we will just convert it back into string (with the format that we want). Then we will make a dictionary and JSON. The function we use is `strftime` (string from time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_log_df['date'] = pd.to_datetime(sub_log_df['date']).dt.strftime('%Y-%m-%d')\n",
    "sub_log_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have everything ready. Let's create the final dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = []\n",
    "for index, row in sub_log_df.iterrows():\n",
    "    logs.append({\n",
    "        \"model\": \"acrelog.operation\", \n",
    "        \"pk\": index+1,\n",
    "        \"fields\": {\n",
    "            \"date\": row['date'],\n",
    "            \"operator\": row['operator'],\n",
    "            \"note\": row['note'],\n",
    "        }\n",
    "    })\n",
    "logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can dump the dictionary to JSON then load the JSON file to the database."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asm591",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
